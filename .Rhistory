if ("Clayton" %in% names(params)) {
slot(copC, "parameters") <- params["Clayton"]
dCop[, 1] <- params["piC"] * copula::dCopula(U, copC)
}
if ("Gumbel" %in% names(params)) {
slot(copG, "parameters") <- params["Gumbel"]
dCop[, 2] <- params["piG"] * copula::dCopula(U, copG)
}
if ("t1" %in% names(params)) {
slot(copT, "parameters") <- params["t"]
dCop[, 3] <- params["piT"] * copula::dCopula(U, copT)
}
if ("Gaussian" %in% names(params)) {
slot(copN, "parameters") <- params["Gaussian"]
dCop[, 4] <- params["piN"] * copula::dCopula(U, copN)
}
if ("Frank" %in% names(params)) {
slot(copF, "parameters") <- params["Frank"]
dCop[, 5] <- params["piF"] * copula::dCopula(U, copF)
}
if ("Joe" %in% names(params)) {
slot(copJ, "parameters") <- params["Joe"]
dCop[, 6] <- params["piJ"] * copula::dCopula(U, copJ)
}
# Remove columns with NaN values
dCop <- dCop[, complete.cases(dCop)]
opt <- log(rowSums(dCop))
# Handle infinite values in the log-likelihood
if(any(is.infinite(opt))){
opt[which(is.infinite(opt))] <- 0
}
# Return the negative sum of the log-likelihood
-sum(opt)
}
# Calculate the log-likelihood function to be optimized
dCop <- matrix(nrow = nrow(U), ncol = 6)
if ("Clayton" %in% names(params)) {
slot(copC, "parameters") <- params["Clayton"]
dCop[, 1] <- params["piC"] * copula::dCopula(U, copC)
}
if ("Gumbel" %in% names(params)) {
slot(copG, "parameters") <- params["Gumbel"]
dCop[, 2] <- params["piG"] * copula::dCopula(U, copG)
}
if ("t1" %in% names(params)) {
slot(copT, "parameters") <- params["t"]
dCop[, 3] <- params["piT"] * copula::dCopula(U, copT)
}
if ("Gaussian" %in% names(params)) {
slot(copN, "parameters") <- params["Gaussian"]
dCop[, 4] <- params["piN"] * copula::dCopula(U, copN)
}
if ("Frank" %in% names(params)) {
slot(copF, "parameters") <- params["Frank"]
dCop[, 5] <- params["piF"] * copula::dCopula(U, copF)
}
if ("Joe" %in% names(params)) {
slot(copJ, "parameters") <- params["Joe"]
dCop[, 6] <- params["piJ"] * copula::dCopula(U, copJ)
}
U = unif_dist
# Calculate the log-likelihood function to be optimized
dCop <- matrix(nrow = nrow(U), ncol = 6)
if ("Clayton" %in% names(params)) {
slot(copC, "parameters") <- params["Clayton"]
dCop[, 1] <- params["piC"] * copula::dCopula(U, copC)
}
if ("Gumbel" %in% names(params)) {
slot(copG, "parameters") <- params["Gumbel"]
dCop[, 2] <- params["piG"] * copula::dCopula(U, copG)
}
if ("t1" %in% names(params)) {
slot(copT, "parameters") <- params["t"]
dCop[, 3] <- params["piT"] * copula::dCopula(U, copT)
}
if ("Gaussian" %in% names(params)) {
slot(copN, "parameters") <- params["Gaussian"]
dCop[, 4] <- params["piN"] * copula::dCopula(U, copN)
}
params["piT"] * copula::dCopula(U, copT)
slot(copT, "parameters") <- params["t1", "t2"]
params
slot(copT, "parameters") <- c(params["t1"], params["t2"])
# Calculate the log-likelihood function to be optimized
dCop <- matrix(nrow = nrow(U), ncol = 6)
if ("Clayton" %in% names(params)) {
slot(copC, "parameters") <- params["Clayton"]
dCop[, 1] <- params["piC"] * copula::dCopula(U, copC)
}
if ("Gumbel" %in% names(params)) {
slot(copG, "parameters") <- params["Gumbel"]
dCop[, 2] <- params["piG"] * copula::dCopula(U, copG)
}
if ("t1" %in% names(params)) {
slot(copT, "parameters") <- c(params["t1"], params["t2"])
dCop[, 3] <- params["piT"] * copula::dCopula(U, copT)
}
if ("Gaussian" %in% names(params)) {
slot(copN, "parameters") <- params["Gaussian"]
dCop[, 4] <- params["piN"] * copula::dCopula(U, copN)
}
if ("Frank" %in% names(params)) {
slot(copF, "parameters") <- params["Frank"]
dCop[, 5] <- params["piF"] * copula::dCopula(U, copF)
}
if ("Joe" %in% names(params)) {
slot(copJ, "parameters") <- params["Joe"]
dCop[, 6] <- params["piJ"] * copula::dCopula(U, copJ)
}
# Remove columns with NaN values
dCop <- dCop[, complete.cases(dCop)]
dCop
logical(dCop)
complete.cases(dCop)
complete.cases(t(dCop))
# Remove columns with NaN values
dCop <- dCop[, complete.cases(t(dCop))]
opt <- log(rowSums(dCop))
LLCG <- function(params, U,
copC, copG, copT, copN, copF, copJ){
# LLCG: Negative log-likelihood function for estimating copula weights and parameters.
# Inputs:
#   params: A numeric vector containing the initial values for copula parameters and weights.
#   U: A matrix containing the uniform (0 to 1) marginals of the data for each copula.
# Output:
#   The negative log-likelihood value to be optimized for estimating copula parameters and weights.
# Calculate the log-likelihood function to be optimized
dCop <- matrix(nrow = nrow(U), ncol = 6)
if ("Clayton" %in% names(params)) {
slot(copC, "parameters") <- params["Clayton"]
dCop[, 1] <- params["piC"] * copula::dCopula(U, copC)
}
if ("Gumbel" %in% names(params)) {
slot(copG, "parameters") <- params["Gumbel"]
dCop[, 2] <- params["piG"] * copula::dCopula(U, copG)
}
if ("t1" %in% names(params)) {
slot(copT, "parameters") <- c(params["t1"], params["t2"])
dCop[, 3] <- params["piT"] * copula::dCopula(U, copT)
}
if ("Gaussian" %in% names(params)) {
slot(copN, "parameters") <- params["Gaussian"]
dCop[, 4] <- params["piN"] * copula::dCopula(U, copN)
}
if ("Frank" %in% names(params)) {
slot(copF, "parameters") <- params["Frank"]
dCop[, 5] <- params["piF"] * copula::dCopula(U, copF)
}
if ("Joe" %in% names(params)) {
slot(copJ, "parameters") <- params["Joe"]
dCop[, 6] <- params["piJ"] * copula::dCopula(U, copJ)
}
# Remove columns with NaN values
dCop <- dCop[, complete.cases(t(dCop))]
opt <- log(rowSums(dCop))
# Handle infinite values in the log-likelihood
if(any(is.infinite(opt))){
opt[which(is.infinite(opt))] <- 0
}
# Return the negative sum of the log-likelihood
-sum(opt)
}
eqfun <- function(params, U,
copC, copG, copT, copN, copF, copJ){
# eqfun: Constrain function to ensure sum of weights = 1.
# Inputs:
#   pi: A numeric vector containing the values of copula weights to be constrained.
# Output:
#   The sum of the copula weights (pi) to be constrained.
z <- params["piC"] + params["piG"] + params["piT"] +
params["piN"] + params["piF"] + params["piJ"]
return(z)
}
# Initialize lower and upper bounds as named vectors
lower <- numeric()
upper <- numeric()
if ("Clayton" %in% names(params)) {
lower["Clayton"] <- 0.1
upper["Clayton"] <- copC@param.upbnd
}
if ("Gumbel" %in% names(params)) {
lower["Gumbel"] <- 1
upper["Gumbel"] <- copG@param.upbnd
}
if ("t1" %in% names(params)) {
lower["t1"] <- -0.9
upper["t1"] <- 1
}
if ("t2" %in% names(params)) {
lower["t2"] <- (2 + .Machine$double.eps)
upper["t2"] <- 100
}
if ("Gaussian" %in% names(params)) {
lower["Gaussian"] <- copN@param.lowbnd
upper["Gaussian"] <- copN@param.upbnd
}
if ("Frank" %in% names(params)) {
lower["Frank"] <- copF@param.lowbnd
upper["Frank"] <- copF@param.upbnd
}
if ("Joe" %in% names(params)) {
lower["Joe"] <- copJ@param.lowbnd
upper["Joe"] <- copJ@param.upbnd
}
if ("Clayton" %in% names(params)) {
lower["piC"] <- 0
upper["piC"] <- 1
}
if ("Gumbel" %in% names(params)) {
lower["piG"] <- 0
upper["piG"] <- 1
}
if ("t1" %in% names(params)) {
lower["piT"] <- 0
upper["piT"] <- 1
}
if ("Gaussian" %in% names(params)) {
lower["piN"] <- 0
upper["piN"] <- 1
}
if ("Frank" %in% names(params)) {
lower["piF"] <- 0
upper["piF"] <- 1
}
if ("Joe" %in% names(params)) {
lower["piJ"] <- 0
upper["piJ"] <- 1
}
## Non-linear constrained optimization (RSOLNP)
opt <- Rsolnp::solnp(pars = params,
fun = LLCG,
LB = lower,
UB = upper,
U = unif_dist,
eqfun = eqfun,
eqB = c(1),
copC = copC,
copG = copG,
copT = copT,
copN = copN,
copF = copF,
copJ = copJ)
opt$pars
## Generating copula variaties
cC <- opt$pars["piC"] * copula::rCopula(n = 10000,
copula = claytonCopula(param = opt$pars["Clayton"],
dim = ncol(unif_dist)))
cG <- opt$pars["piG"] * copula::rCopula(n = 10000,
copula = GumbelCopula(param = opt$pars["Gumbel"],
dim = ncol(unif_dist)))
cG <- opt$pars["piG"] * copula::rCopula(n = 10000,
copula = gumbelCopula(param = opt$pars["Gumbel"],
dim = ncol(unif_dist)))
cT <- opt$pars["piT"] * copula::rCopula(n = 10000,
copula = tCopula(param = opt$pars["t1"],
df = opt$pars["t2"],
dim = ncol(unif_dist)))
cN <- opt$pars["piN"] * copula::rCopula(n = 10000,
copula = GaussianCopula(param = opt$pars["Gaussian"],
dim = ncol(unif_dist)))
cN <- opt$pars["piN"] * copula::rCopula(n = 10000,
copula = gaussianCopula(param = opt$pars["Gaussian"],
dim = ncol(unif_dist)))
cN <- opt$pars["piN"] * copula::rCopula(n = 10000,
copula = normalCopula(param = opt$pars["Gaussian"],
dim = ncol(unif_dist)))
cF <- opt$pars["piF"] * copula::rCopula(n = 10000,
copula = frankCopula(param = opt$pars["Frank"],
dim = ncol(unif_dist)))
cJ <- opt$pars["piJ"] * copula::rCopula(n = 10000,
copula = joeCopula(param = opt$pars["Joe"],
dim = ncol(unif_dist)))
plot(cC)
finalcopula <- cC + cG + cT + cN + cF+ cJ
plot(finalcopula)
finalcopula
plot(finalcopula[,1])
plot(finalcopula[,3])
# setting R project environment
my_dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(my_dir)
# cleaning variables and graphs
rm(list=ls())
graphics.off()
# Load required packages
library(tidyverse)     # Data manipulation and visualization
library(tidyquant)     # Financial data analysis
library(readxl)        # Read excel files
library(rugarch)       # Univariate GARCH modeling
library(fGarch)        # Multivariate GARCH modeling
library(copula)        # Copula modeling
library(Rsolnp)        # Nonlinear optimization
library(fPortfolio)    # Portfolio optimization
library(PerformanceAnalytics) # Performance metrics
library(xts)           # Time series object
library(timetk)        # Time series object
library(xtable)        # Create LaTex tables
library(ggplot2)       # Produce graph
library("ROI")         # Optimization
library("ROI.plugin.glpk") # Optimization
library("ROI.plugin.quadprog") # Optimization
library("ROI.plugin.alabama") # Optimization
# Importing modules
source("data_preprocessing.R")
source("garch_estimate.R")
source("copula_estimate.R")
source("portfolio_optimization.R")
source("portfolio_analysis.R")
source("performance_metrics.R")
source("exporting_results.R")
# Fetch data
# OBSERVATION: CSIP6 was removed from data due to impossibility of found data
Ret <- read_excel("data_directory/StockPrice.xlsx")[-1,] %>%
mutate(date = as.Date(data), # convert to date format
across(VALE3:WHMT3, as.numeric)) %>%  # convert to numeric format
select(date, everything(), -data) %>% # select date and tickers
gather("Ticker", "AdjClose", -date, na.rm = TRUE) %>% # adjust to panel data
group_by(Ticker) %>% # calculate return for each ticker
mutate(Ret = log(AdjClose/ dplyr::lag(AdjClose))) %>%  # compute log return
ungroup() %>% # stop grouping tickers
select(date, Ticker, Ret) %>% # selecting necessary columns
spread(key = "Ticker", value = "Ret", fill = 0) # transforming data
Ret
# Choosing stocks on Ibovespa per year
MarketComposition <- read_excel("data_directory/MarketComposition.xlsx") %>%
mutate(date = as.Date(Data), # convert to date format
m = month(date, label = TRUE)) %>% # convert to numeric format
dplyr::filter(m == "dez") %>% # update factors
select(date, everything(),-c(Data, m)) %>% # select date, month and tickers
gather("Ticker", "Composition", -date, na.rm=TRUE) %>% # adjust to panel data
group_by(date) %>% # Select tickers by last date of the month
reframe(Tickers = Ticker)
MarketComposition
Update <- MarketComposition %>%
select(date) %>%
unique() %>% # avoid repetition
deframe() # create vector
Update
Symbols <- map(Update, .f = function(x){ # list of vectors with Ibov tickers per year
MarketComposition %>%
dplyr::filter(date == x) %>% # choose stocks presented on Ibov
select(Tickers) %>%
unique() %>% # avoid repetition
deframe() # create vector
})
names(Symbols) <- Update
Symbols
# Filtering stocks on Ibovespa and splitting  data set
Ret_inSample <- map(Update, .f = function(x){
Ret %>%
select(date,  Symbols[[paste(x)]]) %>% # filter Ibov stocks
dplyr::filter(date > as.Date(x) - 365, # split data in in-sample
date <= as.Date(x)) %>%
select(-date) %>% # exclude date column
as.matrix() # convert to matrix format
})
names(Ret_inSample) <- Update
Ret_inSample
Ret_outofSample <- map(Update, .f = function(x){
Ret %>%
select(date,  Symbols[[paste(x)]]) %>% # filter Ibov stocks
dplyr::filter(date > as.Date(x), # split data in out-of-sample
date <= as.Date(x) + 365) %>%
select(-date) %>% # exclude date column
as.matrix() # convert to matrix format
})
names(Ret_outofSample) <- Update + 365
Ret_outofSample
# Create returns matrix
returns <- Ret_inSample$`2021-12-31`
# Fit the GARCH model to the returns data
fit_garch <- FitGarch(returns)
# Subset the matrix to keep only columns with complete cases
garch_coef <- Filter(Negate(is.null), fit_garch$garch_coef) # Filtering NULL values
unif_dist <- fit_garch$unif_dist
unif_dist <- unif_dist[, complete.cases(t(unif_dist))] # drop Na columns
sigma <- fit_garch$sigma
returns <- returns[, complete.cases(t(sigma))] # drop invalid stocks
sigma <- sigma[,complete.cases(t(sigma))] # drop Na columns
## Generating Mixture-Copula
copula_mixture <- OptMixtureCopulas(unif_dist,
K = 10000,
combination = c("Frank", "Gumbel"))
# Compute simulated standardized residuals using the optimized mixture-copula and GARCH coefficients
zsim <- ComputeZSim(copula_mixture = copula_mixture,
garch_coef = garch_coef)
# Predict future returns using the GARCH model, simulated residuals, and volatility estimates
ret_pred <- PredictGarch(returns = returns,
sigma = sigma,
zsim = zsim,
garch_coef = garch_coef)
ret_pred <- as.data.frame(ret_pred)
colnames(ret_pred) <- colnames(returns)
# Perform CVaR optimization to determine the optimal portfolio weights
weights <- rep(0, ncol(returns))
names(weights) <- colnames(returns)
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0,
#Turnover = 0.0003,
NumAssets = 8)
# Calculate portfolio returns based on the optimal weights
ret_matrix_outofsample <- Ret_outofSample$`2022-12-31`[,colnames(returns)]
portfolio_returns <- ret_matrix_outofsample  %*%  weights
# Calculate cumulative returns
cumulative_returns <- cumprod(1 + portfolio_returns) - 1
# Plot the cumulative returns
plot(cumulative_returns, type = "l", col = "green", lwd = 2,
main = "Portfolio Performance",
xlab = "Time Period", ylab = "Cumulative Returns")
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0,
#Turnover = 0.0003,
NumAssets = 16)
# Calculate portfolio returns based on the optimal weights
ret_matrix_outofsample <- Ret_outofSample$`2022-12-31`[,colnames(returns)]
portfolio_returns <- ret_matrix_outofsample  %*%  weights
# Calculate cumulative returns
cumulative_returns <- cumprod(1 + portfolio_returns) - 1
# Plot the cumulative returns
plot(cumulative_returns, type = "l", col = "green", lwd = 2,
main = "Portfolio Performance",
xlab = "Time Period", ylab = "Cumulative Returns")
weights
## Generating Mixture-Copula
copula_mixture <- OptMixtureCopulas(unif_dist,
K = 10000,
combination = c("Frank", "Joe"))
# Compute simulated standardized residuals using the optimized mixture-copula and GARCH coefficients
zsim <- ComputeZSim(copula_mixture = copula_mixture,
garch_coef = garch_coef)
# Predict future returns using the GARCH model, simulated residuals, and volatility estimates
ret_pred <- PredictGarch(returns = returns,
sigma = sigma,
zsim = zsim,
garch_coef = garch_coef)
ret_pred <- as.data.frame(ret_pred)
colnames(ret_pred) <- colnames(returns)
# Perform CVaR optimization to determine the optimal portfolio weights
weights <- rep(0, ncol(returns))
names(weights) <- colnames(returns)
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0.03,
NumAssets = 16)
# Calculate portfolio returns based on the optimal weights
ret_matrix_outofsample <- Ret_outofSample$`2022-12-31`[,colnames(returns)]
portfolio_returns <- ret_matrix_outofsample  %*%  weights
# Calculate cumulative returns
cumulative_returns <- cumprod(1 + portfolio_returns) - 1
# Plot the cumulative returns
plot(cumulative_returns, type = "l", col = "green", lwd = 2,
main = "Portfolio Performance",
xlab = "Time Period", ylab = "Cumulative Returns")
cumulative_returns
# Perform CVaR optimization to determine the optimal portfolio weights
weights <- rep(0, ncol(returns))
names(weights) <- colnames(returns)
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0.01,
NumAssets = 16)
# Calculate portfolio returns based on the optimal weights
ret_matrix_outofsample <- Ret_outofSample$`2022-12-31`[,colnames(returns)]
portfolio_returns <- ret_matrix_outofsample  %*%  weights
# Calculate cumulative returns
cumulative_returns <- cumprod(1 + portfolio_returns) - 1
# Plot the cumulative returns
plot(cumulative_returns, type = "l", col = "green", lwd = 2,
main = "Portfolio Performance",
xlab = "Time Period", ylab = "Cumulative Returns")
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0.005,
NumAssets = 16)
# Perform CVaR optimization to determine the optimal portfolio weights
weights <- rep(0, ncol(returns))
names(weights) <- colnames(returns)
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0.005,
NumAssets = 16)
# Calculate portfolio returns based on the optimal weights
ret_matrix_outofsample <- Ret_outofSample$`2022-12-31`[,colnames(returns)]
portfolio_returns <- ret_matrix_outofsample  %*%  weights
# Calculate cumulative returns
cumulative_returns <- cumprod(1 + portfolio_returns) - 1
# Plot the cumulative returns
plot(cumulative_returns, type = "l", col = "green", lwd = 2,
main = "Portfolio Performance",
xlab = "Time Period", ylab = "Cumulative Returns")
weights <- CVaROptimization(returns = ret_pred,
Alpha = 0.05,
TargetReturn = 0,
NumAssets = 16)
# Calculate portfolio returns based on the optimal weights
ret_matrix_outofsample <- Ret_outofSample$`2022-12-31`[,colnames(returns)]
portfolio_returns <- ret_matrix_outofsample  %*%  weights
# Calculate cumulative returns
cumulative_returns <- cumprod(1 + portfolio_returns) - 1
# Plot the cumulative returns
plot(cumulative_returns, type = "l", col = "green", lwd = 2,
main = "Portfolio Performance",
xlab = "Time Period", ylab = "Cumulative Returns")
naive <- NaiveDiversification(ret_matrix_outofsample)
# cleaning variables and graphs
rm(list=ls())
graphics.off()
# cleaning variables and graphs
rm(list=ls())
graphics.off()
