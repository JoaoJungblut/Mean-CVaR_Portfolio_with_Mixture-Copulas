{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date   1996   1997   1998   1999   2000   2001   2002   2003  2004  2005  ...  \\\n",
      "0       AAL    AAL  AAMRQ  AAMRQ   AABA      A      A      A     A     A  ...   \n",
      "1     AAMRQ  AAMRQ   AAPL   AAPL  AAMRQ   AABA   AABA   AABA  AABA  AABA  ...   \n",
      "2      AAPL   AAPL    ABI    ABI   AAPL  AAMRQ  AAMRQ  AAMRQ  AAPL  AAPL  ...   \n",
      "3       ABI    ABI    ABS    ABS    ABI   AAPL   AAPL   AAPL   ABC   ABC  ...   \n",
      "4       ABS    ABS    ABT    ABT    ABS    ABI    ABC    ABC   ABI   ABI  ...   \n",
      "..      ...    ...    ...    ...    ...    ...    ...    ...   ...   ...  ...   \n",
      "501    None   None   None   None   None   None   None   None  None  None  ...   \n",
      "502    None   None   None   None   None   None   None   None  None  None  ...   \n",
      "503    None   None   None   None   None   None   None   None  None  None  ...   \n",
      "504    None   None   None   None   None   None   None   None  None  None  ...   \n",
      "505    None   None   None   None   None   None   None   None  None  None  ...   \n",
      "\n",
      "date  2014  2015  2016  2017  2018  2019  2020  2021  2022  2023  \n",
      "0        A     A     A     A     A     A     A     A     A     A  \n",
      "1     AABA  AABA  AABA  AABA   AAL   AAL   AAL   AAL   AAL   AAL  \n",
      "2     AAPL  AAPL   AAL   AAL   AAP   AAP   AAP   AAP   AAP   AAP  \n",
      "3     ABBV  ABBV   AAP   AAP  AAPL  AAPL  AAPL  AAPL  AAPL  AAPL  \n",
      "4      ABC   ABC  AAPL  AAPL  ABBV  ABBV  ABBV  ABBV  ABBV  ABBV  \n",
      "..     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "501   None  None   ZBH   XYL   YUM   YUM   ZBH   ZBH   ZBH  ZION  \n",
      "502   None  None  ZION   YUM   ZBH   ZBH  ZBRA  ZBRA  ZBRA   ZTS  \n",
      "503   None  None   ZTS   ZBH  ZION  ZION  ZION  ZION  ZION  None  \n",
      "504   None  None  None  ZION   ZTS   ZTS   ZTS   ZTS   ZTS  None  \n",
      "505   None  None  None   ZTS  None  None  None  None  None  None  \n",
      "\n",
      "[506 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "url = \"https://raw.githubusercontent.com/fja05680/sp500/master/S%26P%20500%20Historical%20Components%20%26%20Changes(04-16-2023).csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# split the tickers into separate rows\n",
    "df = df.assign(tickers=df.tickers.str.split(\",\"))\n",
    "df = df.explode(\"tickers\")\n",
    "\n",
    "# create a count column for pivot table\n",
    "df['count'] = 1\n",
    "\n",
    "# create the panel data\n",
    "panel_data = df.pivot_table(index='date', columns='tickers', values='count', aggfunc='sum')\n",
    "panel_data.fillna(0, inplace=True)\n",
    "\n",
    "# melt the data to create a \"long\" format\n",
    "melted_data = pd.melt(panel_data.reset_index(), id_vars='date', var_name='tickers', value_name='value')\n",
    "\n",
    "# select rows where value equal 1\n",
    "melted_data = melted_data[melted_data[\"value\"] == 1]\n",
    "\n",
    "# drop the \"value\" column\n",
    "melted_data.drop('value', axis=1, inplace=True)\n",
    "\n",
    "# convert the date column to datetime format\n",
    "melted_data['date'] = pd.to_datetime(melted_data['date'])\n",
    "\n",
    "# group the tickers by date\n",
    "grouped_data = melted_data.groupby('date')['tickers'].apply(list)\n",
    "\n",
    "# select the row of first date of each year\n",
    "first_dates = grouped_data.groupby(grouped_data.index.year).apply(lambda x: x.iloc[0])\n",
    "\n",
    "# transpose the resulting data\n",
    "transposed_data = pd.DataFrame(first_dates.tolist(), index=first_dates.index)\n",
    "transposed_data = transposed_data.transpose()\n",
    "\n",
    "# display the resulting data transposed\n",
    "print(transposed_data)\n",
    "\n",
    "# save csv file\n",
    "transposed_data.to_csv(\"csv_files/sp500_tickers.csv\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "# read SP500 components \n",
    "df_tickers = pd.read_csv(\"csv_files/sp500_tickers.csv\")\n",
    "df_tickers = df_tickers.rename(columns={\"Unnamed: 0\": \"comp_index\"})\n",
    "\n",
    "# creating a list of all tickers\n",
    "melted_tickers = pd.melt(df_tickers, id_vars=\"comp_index\", var_name=\"year\", value_name=\"ticker\").dropna()\n",
    "tickers = melted_tickers[\"ticker\"].unique().tolist()\n",
    "\n",
    "# api_key \n",
    "api_key1 = \"XQ40GSMB7OJOL6VQ\"\n",
    "api_key2 = \"5X3HSKBVP8LT7590\"\n",
    "api_key3 = \"V1PFJOMDHX76TFF8\"\n",
    "\n",
    "# download the adjusted close price for each ticker\n",
    "data_frames = []\n",
    "missing_symbols = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        # use api_key1 before exceding number of requests in one day\n",
    "        ts = TimeSeries(key=api_key1, output_format='pandas')\n",
    "\n",
    "        print(tickers.index(ticker), ticker)\n",
    "\n",
    "        data, meta_data = ts.get_daily_adjusted(symbol=ticker, outputsize='full')\n",
    "        # rename the column and select only the adjusted close price\n",
    "        data = data[[\"5. adjusted close\"]].rename(columns={\"5. adjusted close\":\"Adj_Close\"})\n",
    "        # create ticker column\n",
    "        data[\"ticker\"] = ticker\n",
    "        # get log-returns\n",
    "        data[\"log_rtn\"] = np.log(data[\"Adj_Close\"]).diff().fillna(0)\n",
    "        # sort the index in ascending order\n",
    "        data = data.sort_index()\n",
    "        # add data to data_frames list \n",
    "        data_frames.append(data)\n",
    "    except ValueError as e:\n",
    "        error_message = str(e)\n",
    "        if \"Invalid API call\" in error_message:\n",
    "            print(f\"Error processing {ticker}: {error_message}\")\n",
    "            # append to removed symbols list\n",
    "            missing_symbols.append(ticker)\n",
    "            # do not append the ticker back to the list\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error processing {ticker}: {error_message}\")\n",
    "            # append the ticker back to the list\n",
    "            tickers.append(ticker)\n",
    "            # wait 1 minute before trying again\n",
    "            time.sleep(60)  \n",
    "    except ConnectionError as e:\n",
    "        # use api_key2 before exceding number of requests in one day\n",
    "        print(f\"Error processing {api_key1}: {error_message}\")\n",
    "        # append the ticker back to the list\n",
    "        tickers.append(ticker)\n",
    "            \n",
    "        # new ts\n",
    "        ts = TimeSeries(key=api_key2, output_format='pandas')\n",
    "\n",
    "        print(tickers.index(ticker), ticker)\n",
    "\n",
    "        try:\n",
    "            data, meta_data = ts.get_daily_adjusted(symbol=ticker, outputsize='full')\n",
    "            # rename the column and select only the adjusted close price\n",
    "            data = data[[\"5. adjusted close\"]].rename(columns={\"5. adjusted close\":\"Adj_Close\"})\n",
    "            # create ticker column\n",
    "            data[\"ticker\"] = ticker\n",
    "            # get log-returns\n",
    "            data[\"log_rtn\"] = np.log(data[\"Adj_Close\"]).diff().fillna(0)\n",
    "            # sort the index in ascending order\n",
    "            data = data.sort_index()\n",
    "            # add data to data_frames list \n",
    "            data_frames.append(data)\n",
    "        except ValueError as e:\n",
    "            error_message = str(e)\n",
    "            if \"Invalid API call\" in error_message:\n",
    "                print(f\"Error processing {ticker}: {error_message}\")\n",
    "                # append to removed symbols list\n",
    "                missing_symbols.append(ticker)\n",
    "                # do not append the ticker back to the list\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error processing {ticker}: {error_message}\")\n",
    "                # append the ticker back to the list\n",
    "                tickers.append(ticker)\n",
    "                # wait 1 minute before trying again\n",
    "                time.sleep(60)  \n",
    "        except ConnectionError as e:\n",
    "             # use api_key3 before exceding number of requests in one day\n",
    "            print(f\"Error processing {api_key2}: {error_message}\")\n",
    "            # append the ticker back to the list\n",
    "            tickers.append(ticker)\n",
    "                \n",
    "            # new ts\n",
    "            ts = TimeSeries(key=api_key3, output_format='pandas')\n",
    "\n",
    "            print(tickers.index(ticker), ticker)\n",
    "\n",
    "            try:\n",
    "                data, meta_data = ts.get_daily_adjusted(symbol=ticker, outputsize='full')\n",
    "                # rename the column and select only the adjusted close price\n",
    "                data = data[[\"5. adjusted close\"]].rename(columns={\"5. adjusted close\":\"Adj_Close\"})\n",
    "                # create ticker column\n",
    "                data[\"ticker\"] = ticker\n",
    "                # get log-returns\n",
    "                data[\"log_rtn\"] = np.log(data[\"Adj_Close\"]).diff().fillna(0)\n",
    "                # sort the index in ascending order\n",
    "                data = data.sort_index()\n",
    "                # add data to data_frames list \n",
    "                data_frames.append(data)\n",
    "            except ValueError as e:\n",
    "                error_message = str(e)\n",
    "                if \"Invalid API call\" in error_message:\n",
    "                    print(f\"Error processing {ticker}: {error_message}\")\n",
    "                    # append to removed symbols list\n",
    "                    missing_symbols.append(ticker)\n",
    "                    # do not append the ticker back to the list\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Error processing {ticker}: {error_message}\")\n",
    "                    # append the ticker back to the list\n",
    "                    tickers.append(ticker)\n",
    "                    # wait 1 minute before trying again\n",
    "                    time.sleep(60)  \n",
    "            except ConnectionError as e:\n",
    "                # use api_key3 before exceding number of requests in one day\n",
    "                print(f\"Error processing {api_key3}: {error_message}\")\n",
    "\n",
    "# concatenate the data frames into a single data frame\n",
    "combined_data = pd.concat(data_frames, axis=0)\n",
    "\n",
    "adj_close_data = combined_data.loc[:, [\"ticker\", \"Adj_Close\"]].reset_index().pivot(index=\"date\", columns=\"ticker\", values=\"Adj_Close\")\n",
    "log_rtn_data = combined_data.loc[:, [\"ticker\", \"log_rtn\"]].reset_index().pivot(index=\"date\", columns=\"ticker\", values=\"log_rtn\")\n",
    "\n",
    "# print tickers not found\n",
    "missing_symbols_df = pd.DataFrame({\"Symbols\": missing_symbols})\n",
    "print(missing_symbols)\n",
    "\n",
    "# print the first and last 5 rows of the data \n",
    "print(adj_close_data.head())\n",
    "print(log_rtn_data.tail())\n",
    "\n",
    "# save csv file\n",
    "missing_symbols_df.to_csv(\"csv_files/missing_symbols.csv\")\n",
    "adj_close_data.to_csv(\"csv_files/adj_close.csv\")\n",
    "log_rtn_data.to_csv(\"csv_files/log_rtn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
