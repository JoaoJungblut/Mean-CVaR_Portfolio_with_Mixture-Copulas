{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "url = \"https://raw.githubusercontent.com/fja05680/sp500/master/S%26P%20500%20Historical%20Components%20%26%20Changes(04-16-2023).csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# split the tickers into separate rows\n",
    "df = df.assign(tickers=df.tickers.str.split(\",\"))\n",
    "df = df.explode(\"tickers\")\n",
    "\n",
    "# create a count column for pivot table\n",
    "df['count'] = 1\n",
    "\n",
    "# create the panel data\n",
    "panel_data = df.pivot_table(index='date', columns='tickers', values='count', aggfunc='sum')\n",
    "panel_data.fillna(0, inplace=True)\n",
    "\n",
    "# melt the data to create a \"long\" format\n",
    "melted_data = pd.melt(panel_data.reset_index(), id_vars='date', var_name='tickers', value_name='value')\n",
    "\n",
    "# select rows where value equal 1\n",
    "melted_data = melted_data[melted_data[\"value\"] == 1]\n",
    "\n",
    "# drop the \"value\" column\n",
    "melted_data.drop('value', axis=1, inplace=True)\n",
    "\n",
    "# convert the date column to datetime format\n",
    "melted_data['date'] = pd.to_datetime(melted_data['date'])\n",
    "\n",
    "# group the tickers by date\n",
    "grouped_data = melted_data.groupby('date')['tickers'].apply(list)\n",
    "\n",
    "# select the row of first date of each year\n",
    "first_dates = grouped_data.groupby(grouped_data.index.year).apply(lambda x: x.iloc[0])\n",
    "\n",
    "# transpose the resulting data\n",
    "transposed_data = pd.DataFrame(first_dates.tolist(), index=first_dates.index)\n",
    "transposed_data = transposed_data.transpose()\n",
    "\n",
    "# display the resulting data transposed\n",
    "print(transposed_data)\n",
    "\n",
    "# save csv file\n",
    "transposed_data.to_csv('sp500_components.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "# read SP500 components \n",
    "df_tickers = pd.read_csv(\"sp500_components.csv\")\n",
    "df_tickers = df_tickers.rename(columns={\"Unnamed: 0\": \"comp_index\"})\n",
    "\n",
    "# creating a list of all tickers\n",
    "melted_tickers = pd.melt(df_tickers, id_vars=\"comp_index\", var_name=\"year\", value_name=\"ticker\").dropna()\n",
    "tickers = melted_tickers[\"ticker\"].unique().tolist()\n",
    "\n",
    "# api_key \n",
    "api_key1 = \"XQ40GSMB7OJOL6VQ\"\n",
    "api_key2 = \"5X3HSKBVP8LT7590\"\n",
    "api_key3 = \"V1PFJOMDHX76TFF8\"\n",
    "\n",
    "# download the adjusted close price for each ticker\n",
    "data_frames = []\n",
    "missing_symbols = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        # use api_key1 before exceding number of requests in one day\n",
    "        ts = TimeSeries(key=api_key1, output_format='pandas')\n",
    "\n",
    "        print(tickers.index(ticker), ticker)\n",
    "\n",
    "        data, meta_data = ts.get_daily_adjusted(symbol=ticker, outputsize='full')\n",
    "        # rename the column and select only the adjusted close price\n",
    "        data = data[[\"5. adjusted close\"]].rename(columns={\"5. adjusted close\":\"Adj_Close\"})\n",
    "        # create ticker column\n",
    "        data[\"ticker\"] = ticker\n",
    "        # get log-returns\n",
    "        data[\"log_rtn\"] = np.log(data[\"Adj_Close\"]).diff().fillna(0)\n",
    "        # sort the index in ascending order\n",
    "        data = data.sort_index()\n",
    "        # add data to data_frames list \n",
    "        data_frames.append(data)\n",
    "    except ValueError as e:\n",
    "        error_message = str(e)\n",
    "        if \"Invalid API call\" in error_message:\n",
    "            print(f\"Error processing {ticker}: {error_message}\")\n",
    "            # append to removed symbols list\n",
    "            missing_symbols.append(ticker)\n",
    "            # do not append the ticker back to the list\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error processing {ticker}: {error_message}\")\n",
    "            # append the ticker back to the list\n",
    "            tickers.append(ticker)\n",
    "            # wait 1 minute before trying again\n",
    "            time.sleep(60)  \n",
    "    except ConnectionError as e:\n",
    "        # use api_key2 before exceding number of requests in one day\n",
    "        print(f\"Error processing {api_key1}: {error_message}\")\n",
    "        # append the ticker back to the list\n",
    "        tickers.append(ticker)\n",
    "            \n",
    "        # new ts\n",
    "        ts = TimeSeries(key=api_key2, output_format='pandas')\n",
    "\n",
    "        print(tickers.index(ticker), ticker)\n",
    "\n",
    "        try:\n",
    "            data, meta_data = ts.get_daily_adjusted(symbol=ticker, outputsize='full')\n",
    "            # rename the column and select only the adjusted close price\n",
    "            data = data[[\"5. adjusted close\"]].rename(columns={\"5. adjusted close\":\"Adj_Close\"})\n",
    "            # create ticker column\n",
    "            data[\"ticker\"] = ticker\n",
    "            # get log-returns\n",
    "            data[\"log_rtn\"] = np.log(data[\"Adj_Close\"]).diff().fillna(0)\n",
    "            # sort the index in ascending order\n",
    "            data = data.sort_index()\n",
    "            # add data to data_frames list \n",
    "            data_frames.append(data)\n",
    "        except ValueError as e:\n",
    "            error_message = str(e)\n",
    "            if \"Invalid API call\" in error_message:\n",
    "                print(f\"Error processing {ticker}: {error_message}\")\n",
    "                # append to removed symbols list\n",
    "                missing_symbols.append(ticker)\n",
    "                # do not append the ticker back to the list\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error processing {ticker}: {error_message}\")\n",
    "                # append the ticker back to the list\n",
    "                tickers.append(ticker)\n",
    "                # wait 1 minute before trying again\n",
    "                time.sleep(60)  \n",
    "        except ConnectionError as e:\n",
    "             # use api_key3 before exceding number of requests in one day\n",
    "            print(f\"Error processing {api_key2}: {error_message}\")\n",
    "            # append the ticker back to the list\n",
    "            tickers.append(ticker)\n",
    "                \n",
    "            # new ts\n",
    "            ts = TimeSeries(key=api_key3, output_format='pandas')\n",
    "\n",
    "            print(tickers.index(ticker), ticker)\n",
    "\n",
    "            try:\n",
    "                data, meta_data = ts.get_daily_adjusted(symbol=ticker, outputsize='full')\n",
    "                # rename the column and select only the adjusted close price\n",
    "                data = data[[\"5. adjusted close\"]].rename(columns={\"5. adjusted close\":\"Adj_Close\"})\n",
    "                # create ticker column\n",
    "                data[\"ticker\"] = ticker\n",
    "                # get log-returns\n",
    "                data[\"log_rtn\"] = np.log(data[\"Adj_Close\"]).diff().fillna(0)\n",
    "                # sort the index in ascending order\n",
    "                data = data.sort_index()\n",
    "                # add data to data_frames list \n",
    "                data_frames.append(data)\n",
    "            except ValueError as e:\n",
    "                error_message = str(e)\n",
    "                if \"Invalid API call\" in error_message:\n",
    "                    print(f\"Error processing {ticker}: {error_message}\")\n",
    "                    # append to removed symbols list\n",
    "                    missing_symbols.append(ticker)\n",
    "                    # do not append the ticker back to the list\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Error processing {ticker}: {error_message}\")\n",
    "                    # append the ticker back to the list\n",
    "                    tickers.append(ticker)\n",
    "                    # wait 1 minute before trying again\n",
    "                    time.sleep(60)  \n",
    "            except ConnectionError as e:\n",
    "                # use api_key3 before exceding number of requests in one day\n",
    "                print(f\"Error processing {api_key3}: {error_message}\")\n",
    "\n",
    "# concatenate the data frames into a single data frame\n",
    "combined_data = pd.concat(data_frames, axis=0)\n",
    "\n",
    "adj_close_data = combined_data.loc[:, [\"ticker\", \"Adj_Close\"]].reset_index().pivot(index=\"date\", columns=\"ticker\", values=\"Adj_Close\")\n",
    "log_rtn_data = combined_data.loc[:, [\"ticker\", \"log_rtn\"]].reset_index().pivot(index=\"date\", columns=\"ticker\", values=\"log_rtn\")\n",
    "\n",
    "# print tickers not found\n",
    "missing_symbols_df = pd.DataFrame({\"Symbols\": missing_symbols})\n",
    "print(missing_symbols)\n",
    "\n",
    "# print the first and last 5 rows of the data \n",
    "print(adj_close_data.head())\n",
    "print(log_rtn_data.tail())\n",
    "\n",
    "# save csv file\n",
    "missing_symbols_df.to_csv(\"missing_symbols.csv\")\n",
    "adj_close_data.to_csv(\"adj_close.csv\")\n",
    "log_rtn_data.to_csv(\"log_rtn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
